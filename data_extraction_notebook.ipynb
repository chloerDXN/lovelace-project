{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meet the meat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "With increasingly dire climate change forecasts, concerned individuals are asking how they can minimize their carbon footprint. Recent research suggests that reducing one's consumption of meat, in particular beef, is one of the highest impact actions an individual can take. To examine this topic, we will explore the popularity and prevalence of meat in recipes. Specifically, we plan to extract the ingredients from a recipe database and calculate the carbon footprint of recipes\n",
    "\n",
    "Finally, we hope to directly relate this data to the issue of climate change by estimating a rating reflecting the carbon footprint of meat in recipes and the environmental impact of consumers' diets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import os, os.path as osp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER='data'\n",
    "SAMPLE_DATA_FOLDER = DATA_FOLDER + '/recipePages/'#'/sample_400/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define carbon footprint of meat ingredients\n",
    "Animal agriculture is one of the leading sources of the carbon-impact of a recipe. We start by assigning a carbon footprint to each meat ingredient and could later on extend it to other animal products. \n",
    "The functions below assign a carbon footprint to each meat ingredient of the recipes.\n",
    "\n",
    "Source of data: [GreenEatz](https://www.greeneatz.com/foods-carbon-footprint.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Food</th>\n",
       "      <th>CO2 Kilos Equivalent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lamb</td>\n",
       "      <td>39.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>beef</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cheese</td>\n",
       "      <td>13.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pork</td>\n",
       "      <td>12.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>turkey</td>\n",
       "      <td>10.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>chicken</td>\n",
       "      <td>6.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tuna</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>egg</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Food  CO2 Kilos Equivalent\n",
       "Rank                               \n",
       "1        lamb                  39.2\n",
       "2        beef                  27.0\n",
       "3      cheese                  13.5\n",
       "4        pork                  12.1\n",
       "5      turkey                  10.9\n",
       "6     chicken                   6.9\n",
       "7        tuna                   6.1\n",
       "8         egg                   4.8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data from xls file\n",
    "carbon_footprint_list = pd.read_excel('data/carbon_footprint_protein.xls', sheet_name='meat_dairy_eggs', index_col=0)\n",
    "carbon_footprint_list = carbon_footprint_list[['Food','CO2 Kilos Equivalent']]\n",
    "carbon_footprint_list['Food']=['lamb','beef','cheese','pork','turkey','chicken','tuna','egg']\n",
    "carbon_footprint_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of meat ingredients: ingredients of interest that we want to look for in our recipes\n",
    "meat_products = ['steak','lamb', 'beef', 'cheese', 'pork', 'turkey', 'chicken', 'tuna', 'egg']\n",
    "#units to recognize\n",
    "units = ['pound','gram','oz','ounce','kg','kilogram','lb', 'egg' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data extraction and cleaning\n",
    "\n",
    "Our recipe dataset contains recipes from the [From Cookies to Cooks](http://infolab.stanford.edu/~west1/from-cookies-to-cooks/), combining recipes from 14 high-traffic websites. We start by extracting all the information we want from the HTML files, that is: title, ingredients and meat or animal protein ingredients, tags, ratings in order to explore the recipes in more detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recipe webpage scraping\n",
    "The websites' HTML sources are rich in information. However, the information we wantfrom these pages is rather limited. We extract the information we need from the websites, clean and pre-process the data and save it as a CSV file for easy retrieval in further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "#UPDATE:\n",
    "#I(Alex) changed the website values so that they are compatible when extracting the website name from the title\n",
    "#See the function \"tolkenize_title\"\n",
    "#####################\n",
    "\n",
    "def find_website(soup):\n",
    "    \"\"\"\n",
    "    Finds if the page is a recipe and which website it comes from\n",
    "    \"\"\"\n",
    "    is_recipe = True\n",
    "    \n",
    "    if 'allrecipes' in soup.title.string.casefold():\n",
    "        website = 'allrecipes'               \n",
    "              \n",
    "    elif 'epicurious' in soup.title.string.casefold():\n",
    "        website = 'epicurious'\n",
    "    \n",
    "    elif 'food network' in soup.title.string.casefold():\n",
    "        website = 'food network'\n",
    "        \n",
    "    elif 'food.com' in soup.title.string.casefold():\n",
    "        website == 'food.com'\n",
    "    \n",
    "    elif 'betty crocker' in soup.title.string.casefold() or 'bettycrocker' in soup.title.string.casefold():\n",
    "        website = 'betty crocker'\n",
    "               \n",
    "    elif 'myrecipes' in soup.title.string.casefold():\n",
    "        website = 'myrecipes'\n",
    "    \n",
    "    elif 'taste.com' in soup.title.string.casefold():\n",
    "        website = 'taste'\n",
    "\n",
    "    else:\n",
    "        website = 'not found'\n",
    "        is_recipe = False\n",
    "        \n",
    "    return is_recipe, website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantity extraction and conversion\n",
    "The amounts of each ingredients are expressed in many different units (imperial or metric) depending on the websites, and even on the recipes. Once we have extracted the ingredients and amounts, we need to convert all different quantities to one single weight unit (fixed to kilograms) in order to process the carbon footprint of selected ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_page(soup, page):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        soup\n",
    "        page: 'allrecipes', 'epicurious', 'food network', 'food.com', 'betty crocker', 'myrecipes' , others not implemented yet\n",
    "    \n",
    "    Output:\n",
    "        tags = list of tags assigned to the recipe\n",
    "        ings = list of ingredients\n",
    "    \"\"\"\n",
    "    ings = []\n",
    "    ing_wrap=None\n",
    "    tags = []\n",
    "    rate = None\n",
    "    serv = None\n",
    "    \n",
    "    if page == 'allrecipes':\n",
    "        # Extract tags\n",
    "        #tag_wrappers = soup.find_all(itemprop=\"recipeCategory\") \n",
    "        #for tag in tag_wrappers:\n",
    "        #    tags.append(tag['content'])           \n",
    "        #if not tag_wrappers:\n",
    "        #    tag_wrappers = soup.find_all('li', class_=\"leftnav-middle-title\")\n",
    "            #for lis in soup.find_all('li','ul'):# class_='leftnav-middle-title'):\n",
    "        #    for tag in tag_wrappers:\n",
    "        #        if tag.h4.contents[0]==\"Related Collections\":\n",
    "        #            print('Collections found')\n",
    "        #            tags.append(tag['content'])\n",
    "        #            print(tag)\n",
    "                    \n",
    "        # Extract ingredients\n",
    "        try:\n",
    "            ing_wrap=soup.find_all('li', class_=\"plaincharacterwrap ingredient\")#1st try\n",
    "            if ing_wrap:\n",
    "                for ing in ing_wrap:\n",
    "                    ings.append(ing.getText())\n",
    "            else:\n",
    "                ing_wrap=soup.find_all(itemprop=\"recipeIngredient\") #2nd try\n",
    "                for ing in ing_wrap:\n",
    "                    ings.append(ing.getText()) \n",
    "        except:\n",
    "            pass\n",
    "            #print('Ingredients could not be extracted from '+ page +\" recipe\")\n",
    "\n",
    "        # Extract Ratings\n",
    "        try:\n",
    "            rate_wrapper=soup.find('p',class_='reviewP')\n",
    "            rate_wrapper=rate_wrapper.find('img', id=\"ctl00_CenterColumnPlaceHolder_recipe_ratingStuff_imgRating\")\n",
    "            rate=re.findall(r\"[+]?\\d*\\.\\d+|\\d+\", rate_wrapper['title'])[0]\n",
    "        except:\n",
    "            pass\n",
    "            #print('Ratings could not be extracted from '+ page +\" recipe\")\n",
    "\n",
    "        # Extract Number of servings\n",
    "        try:\n",
    "            serv=soup.find('span', class_=\"yield yieldform\").text\n",
    "        except:\n",
    "            pass\n",
    "            #print('Serving size could not be extracted from '+ page +\" recipe\")\n",
    "\n",
    "        \n",
    "    elif page == 'epicurious':       \n",
    "        # Extract tags\n",
    "\n",
    "        #tag_wrappers = soup.find_all(itemprop=\"recipeCuisine\")\n",
    "        #for tag in tag_wrappers:\n",
    "        #    tags.append(tag.getText())    \n",
    "        #tag_wrappers = soup.find_all(itemprop=\"recipeCategory\")\n",
    "        #for tag in tag_wrappers:\n",
    "        #    tags.append(tag.getText())        \n",
    "        \n",
    "        # Extract ingredients\n",
    "        try:            \n",
    "            ing_wrap=soup.find('div', id=\"ingredients\")\n",
    "            if ing_wrap:\n",
    "                for ing in ing_wrap:\n",
    "                    ings.append(ing.string)\n",
    "            if None in ings:\n",
    "                ings=[]\n",
    "                ing_wrap=soup.find_all('li', class_=\"ingredient\")\n",
    "                for ing in ing_wrap:\n",
    "                    ings.append(ing.string)\n",
    "        except:\n",
    "            pass\n",
    "            #print('Ingredients could not be extracted from '+ page +\" recipe\")\n",
    "        #extract rating\n",
    "        try:\n",
    "            rate_wrapper=soup.find('p',id='prepare_again_rating') \n",
    "            rate=re.findall(r\"[+]?\\d*\\.\\d+|\\d+\", rate_wrapper.getText())[0]\n",
    "            rate=float(rate)*5/100\n",
    "        except:\n",
    "            pass\n",
    "            #print('Rating could not be extracted from '+ page +\" recipe\")\n",
    "\n",
    "            #extract serving size \n",
    "        try:\n",
    "            serv=soup.find('span',class_='yield').text\n",
    "        except:\n",
    "            pass\n",
    "            #print('Serving size could not be extracted from '+ page +\" recipe\")\n",
    "\n",
    "    elif page == 'food network':  \n",
    "        # Extract tags\n",
    "        #tag_wrappers = soup.find_all(class_=\"btn grey-tags\")\n",
    "        #if not tag_wrappers:\n",
    "        #    tag_wrappers = soup.find_all(class_='tag')\n",
    "        #for tag in tag_wrappers:   ##THIS ONE?\n",
    "        #    tags.append(tag.getText())\n",
    "            \n",
    "        #for t in soup.find_all(id = \"recipe-filedin\"):  #OR THIS ONE?\n",
    "        #    for li in t.find_all('li'):\n",
    "        #        for a in li.find_all('a'):\n",
    "        #            tags.append(a.getText())\n",
    "        \n",
    "        #for tag in tag_wrappers:\n",
    "        #    tags.append(tag.getText())      \n",
    "        \n",
    "        # Extract ingredients\n",
    "        try:\n",
    "            ing_wrap=soup.find_all('li',class_='ingredient')\n",
    "            for ing in ing_wrap:\n",
    "                ings.append(ing.text)\n",
    "        except:\n",
    "            pass\n",
    "            #print('Ingredients could not be extracted from '+ page +\" recipe\")\n",
    "\n",
    "        #Extract rating\n",
    "        try:\n",
    "            rate_wrapper=soup.find('div', class_='rm-block lead hreview-aggregate review')\n",
    "            rate=rate_wrapper.find('div')['title']\n",
    "        except:\n",
    "            pass\n",
    "            #print('Rating could not be extracted from '+ page +\" recipe\")\n",
    "            \n",
    "        #extract serving size\n",
    "        try:\n",
    "            serv0=soup.find('div', id='recipe-meta')\n",
    "            serv1=serv0.find_all('dd', class_=\"clrfix\")\n",
    "            for ser in serv1:\n",
    "                if any(char.isdigit() for char in ser.get_text()):\n",
    "                #'serving' in ser.get_text() or 'cup' in ser.get_text():  ##NEED A BETTER WAY TO DO THIS (00a1cb2c972a31e50718971edd070a50)\n",
    "                    serv=ser.get_text()\n",
    "        except:\n",
    "            pass\n",
    "            #print('Serving could not be extracted from '+ page +\" recipe\")\n",
    "\n",
    "    elif page == 'food.com':      \n",
    "        # Extract tags\n",
    "            #not found          \n",
    "        # Extract ingredients\n",
    "        try:\n",
    "            ing_wrap=soup.find_all('li', class_=\"ingredient\")\n",
    "            if ing_wrap:\n",
    "                for ing in ing_wrap:\n",
    "                    ings.append((ing.find('span',class_='value').text+ ' '+ing.find('span',class_='type').text + ' ' + ing.find('span', class_='name').text))\n",
    "            else:\n",
    "                ing_wrap=soup.find_all(class_=\"name\")\n",
    "                for ing in ing_wrap:\n",
    "                    ings.append(ing.getText())\n",
    "        except:\n",
    "            pass\n",
    "            #print('Ingredients could not be extracted from '+ page +\" recipe\")\n",
    "\n",
    "\n",
    "    elif page == 'betty crocker':   \n",
    "        # Extract tags\n",
    "            #not found    \n",
    "            \n",
    "        # Extract ingredients\n",
    "        try:\n",
    "            ing_wrap=soup.find_all('dl', class_='ingredient')\n",
    "            for ing in ing_wrap:\n",
    "                ings.append(ing.getText())\n",
    "        except:\n",
    "            pass\n",
    "            #print('Ingredients could not be extracted from '+ page +\" recipe\")\n",
    "\n",
    "        #Extract serving size\n",
    "        try:\n",
    "            serv=(soup.find('meta', itemprop='recipeYield'))['content'] #tag attribute 'content'\n",
    "        except:\n",
    "            pass\n",
    "            #print('Serving size could not be extracted from '+ page +\" recipe\")\n",
    "\n",
    "    elif page == 'myrecipes':\n",
    "        # Extract tags\n",
    "        #tag_wrappers = soup.find_all(itemprop=\"recipeType\")\n",
    "        #for tag in tag_wrappers:\n",
    "        #    tags.append(tag.getText())  \n",
    "            \n",
    "        # Extract ingredients\n",
    "        try:\n",
    "            ing_wrap=soup.find_all(itemprop=\"ingredient\")\n",
    "            for ing in ing_wrap:\n",
    "                ings.append(ing.text)\n",
    "        except:\n",
    "            pass\n",
    "            #print('Ingredients could not be extracted from '+ page +\" recipe\")\n",
    "\n",
    "        #extract serving size\n",
    "        try:\n",
    "            serv=soup.find('span', itemprop='yield').text\n",
    "        except:\n",
    "            pass\n",
    "            #print('Serving size could not be extracted from '+ page +\" recipe\")\n",
    "\n",
    "    elif page == 'taste':\n",
    "        #Extract ingredients\n",
    "        try:\n",
    "            ing_wrap = soup.find('div', class_=\"module recipe-ingredients\")\n",
    "            ing_wrap=ing_wrap.find_all('li')\n",
    "            #print(ing_wrap)\n",
    "            for ing in ing_wrap:\n",
    "                ings.append(ing.text)\n",
    "        except:\n",
    "            pass\n",
    "            #print('Ingredients could not be extracted from '+ page +\" recipe\")\n",
    "        \n",
    "        try:\n",
    "            rate_wrapper=soup.find('span', itemprop='rating')\n",
    "            rate=rate_wrapper.getText()\n",
    "        except:\n",
    "            pass\n",
    "            #print('Rating could not be extracted from '+ page +\" recipe\")\n",
    "\n",
    "        \n",
    "        #extract serving size\n",
    "        try:\n",
    "            serv=soup.find('h2', class_=\"ingredients\").text\n",
    "        except: \n",
    "            pass\n",
    "            #print('Serving size could not be extracted from '+ page +\" recipe\")\n",
    "\n",
    "            \n",
    "    #other websites    \n",
    "        # Extract tags   \n",
    "        # Extract ingredients \n",
    "        \n",
    "    #if not ing_wrap:  #return warning if website is recognized but format/data extraction is not successful\n",
    "    #    print('Not a recipe format')  \n",
    "    #    print('*******')\n",
    "    \n",
    "    #if not tags:\n",
    "        #print('no tags found :( ')\n",
    "        \n",
    "    return tags, ings, rate, serv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_quantity(quant_str):\n",
    "    \"\"\"\n",
    "    Cleans input string and extracts numerical values\n",
    "    Outputs cleaned string, array of numerical values and sum of numerical values\n",
    "    \"\"\"\n",
    "    quant_str=quant_str.replace(\"½\",\".5\")\n",
    "    quant_str=quant_str.replace(\"1/2\",\".5\")\n",
    "    quant_str=quant_str.replace(\"1/3\", '.33')\n",
    "    quant_str=quant_str.replace('1/4','.25')\n",
    "    quant_str=quant_str.replace('3/4','.75')\n",
    "    \n",
    "\n",
    "    if 'dozen' in quant_str: #likely going to be in the form \"2 dozen\"\n",
    "        \n",
    "        quant_str=quant_str.replace('dozen', 'x12')\n",
    "        \n",
    "        #matches positive decimals or whole numbers\n",
    "        quant_vals=re.findall(r\"[+]?\\d*\\.\\d+|\\d+\", quant_str)\n",
    "        total_quant=np.prod([float(i) for i in quant_vals])\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        #matches positive decimals or whole numbers\n",
    "        quant_vals=re.findall(r\"[+]?\\d*\\.\\d+|\\d+\", quant_str)\n",
    "        total_quant=np.sum([float(i) for i in quant_vals])    \n",
    "\n",
    "    return quant_str, quant_vals, total_quant\n",
    "\n",
    "\n",
    "def convert_to_kg(quant, unit):\n",
    "    \"\"\"\n",
    "    Converts any input unit (kg, lb, grams, ounces) to kilograms\n",
    "    \"\"\"\n",
    "    \n",
    "    if (unit=='kilogram') or (unit=='kg'):\n",
    "        amnt_kg=quant\n",
    "        #print(quant,'kg')\n",
    "    elif (unit=='pound') or (unit=='lb') or (unit=='lbs') or (unit=='pounds'):\n",
    "        amnt_kg=quant/2.205\n",
    "        #print(amnt_kg,'kg')\n",
    "    elif(unit=='g') or (unit=='gram') or (unit =='grams'):\n",
    "        amnt_kg=quant/1000\n",
    "        #print(amnt_kg,'kg')     \n",
    "    elif(unit=='oz') or (unit=='ounce'):\n",
    "        amnt_kg=quant/35.274\n",
    "        #print(amnt_kg, 'kg')\n",
    "    elif(unit=='egg'):\n",
    "        amnt_kg=quant*0.006 #1 egg weighs aproximately 60g\n",
    "    else:\n",
    "        pass\n",
    "        #print('unit not recognized')\n",
    "        \n",
    "    return(amnt_kg)\n",
    "\n",
    "def contains_meat_ingredients(ings_in, meat_products_in):\n",
    "    contains_meat=False\n",
    "    meat_ingredients=[]\n",
    "    meat_ingredients_str=[]\n",
    "    for i in ings_in:\n",
    "        for meat_product in meat_products_in:\n",
    "            if i != None:\n",
    "                if meat_product in i.casefold(): \n",
    "                    contains_meat=True\n",
    "                    meat_ingredients.append(meat_product) \n",
    "                    meat_ingredients_str.append(i)\n",
    "                    \n",
    "    return contains_meat, meat_ingredients, meat_ingredients_str\n",
    "\n",
    "def extract_meat(meat_ings_str):\n",
    "    \"\"\"\n",
    "    Inputs: \n",
    "    ings_in= list of ingredients (and quantities)\n",
    "     \n",
    "    Outputs:\n",
    "    ing_amnt_out = list of corresponding quanities of meat ingredients in kg (=0 if unit not recognized)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    ing_amnt_out=[]\n",
    "    #extract amount from string and convert to kg\n",
    "    for meat_i in meat_ings_str:\n",
    "        meat_i_quant_kg=0\n",
    "        meat_i, quantity_vals, total_quantity=check_quantity(meat_i) #pass string, return cleaned string and total quantity\n",
    "        #find appropriate units and convert to kg\n",
    "        for u in units:\n",
    "            if u in meat_i.casefold():\n",
    "                meat_i_quant_kg = convert_to_kg(total_quantity,u)\n",
    "        ing_amnt_out.append(meat_i_quant_kg)\n",
    "        #if meat_i_quant_kg==0:\n",
    "        #    print('Units not recognized for: '+meat_i)\n",
    "                    \n",
    "    return ing_amnt_out\n",
    "\n",
    "def normalize_servings(ing_amount, servs):\n",
    "    #input is ingredient quantity in kg\n",
    "    #interpret servings as a total quantity using the check_quantity function\n",
    "    \n",
    "    ing_norm=[]\n",
    "    str_servs, val_servs, tot_servs = check_quantity(servs)\n",
    "    for ing in ing_amount:\n",
    "        ing_norm.append(ing/tot_servs)\n",
    "    #print('normalizing by number of servings: ', tot_servs)\n",
    "    return ing_norm    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     Food  carbon footprint\n",
       " 0    beef            51.030\n",
       " 1  cheese             4.050\n",
       " 2     egg             0.096, 55.175999999999995)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate carbon footprint\n",
    "def compute_carbon_fp (meat_ingred, meat_quant, meat_carbon_list):\n",
    "    \"\"\"\n",
    "    \n",
    "    #SHOULD WE USE A CONSTANT WEIGHT IF IT CONTAINS THE INGREDIENT AND THEN A WEIGHT DEPENDING ON HOW MUCH IT CONTAINS?\n",
    "    #ALTERNATIVELY WE COULD ESTIMATE HOW MUCH OF AN INGEDIENT IS BASED ON THE MEDIAN VALUE\n",
    "    #OR DROP ALL RECIPES WHERE WE DON'T KNOW THE QUANTITY\n",
    "    \n",
    "    \n",
    "    input: \n",
    "    meat_ingred= types of meat in recipe\n",
    "    meat_quant = quantity (normalized by serving size) of meat ingredient in kg\n",
    "    meat_carbon_list = carbon footprint/kg of ingredients of interest\n",
    "    output:\n",
    "    df_merge = dataframe with the ingredient type and its corresponding carbon footprint\n",
    "    carbon_footprint (sum of all ingredients' carbon footprint)\n",
    "    \n",
    "    \"\"\"\n",
    "    for i in np.arange(len(meat_ingred)):\n",
    "        if 'steak' in meat_ingred[i].casefold():\n",
    "            meat_ingred[i]='beef'\n",
    "            \n",
    "    #create data frame and merge with carbon footprint\n",
    "    df=pd.DataFrame({'meat item':meat_ingred, 'item quantity':meat_quant})        \n",
    "    df=df.groupby(df['meat item']).sum()\n",
    "    df_merge=df.merge(meat_carbon_list, how='left', left_on='meat item', right_on='Food')\n",
    "    df_merge['carbon footprint']=df_merge['item quantity']*df_merge['CO2 Kilos Equivalent']\n",
    "    #print(df_merge)\n",
    "    return df_merge[['Food','carbon footprint']], df_merge['carbon footprint'].sum()\n",
    "    \n",
    "\n",
    "compute_carbon_fp(['beef','egg','cheese', 'steak'], [1.5,.02,.3,.39], carbon_footprint_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print iterations progress\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█'):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = '\\r')\n",
    "    # Print New Line on Complete\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data extraction and cleaning loop\n",
    "Below we extract the data from the recipes of our html dataset and save it in dataframes. Our goal here is to extract the ingredients and assign a carbon-impact rating to the highest impact ingredients (meat or animal protein) in the recipes.\n",
    "\n",
    "To extract protein-rich ingredients from animal source in order to calculate the main carbon footprint of the recipe, we use an extra database listing the main protein sources and carbon impact. Source of data: [GreenEatz](https://www.greeneatz.com/foods-carbon-footprint.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |--------------------------------------------------| 1.9% Complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexk\\Anaconda\\envs\\ada\\lib\\site-packages\\ipykernel\\__main__.py:101: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |█-------------------------------------------------| 3.9% Complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexk\\Anaconda\\envs\\ada\\lib\\site-packages\\ipykernel\\__main__.py:101: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |███████████████-----------------------------------| 31.5% Complete\r"
     ]
    }
   ],
   "source": [
    "#Loop for all recipes in folder\n",
    "# data has following row structure\n",
    "# RecipeName as Identifier - bool contains_meat - list of co2 ingredients - carbonFootprint - ingredients\n",
    "data=[]\n",
    "step=0\n",
    "total_num=93701\n",
    "\n",
    "verbose = 0 #verbose outputs\n",
    "printProgressBar(0, total_num, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
    "\n",
    "for filename in os.listdir(SAMPLE_DATA_FOLDER):\n",
    "    with open(SAMPLE_DATA_FOLDER+filename) as f:\n",
    "        isTrue=False\n",
    "        count_exceptions=0\n",
    "\n",
    "        try:\n",
    "            page = f.read()\n",
    "            soup = BeautifulSoup(page, 'html.parser')\n",
    "            \n",
    "            #check webpage and extract ingredients if recognised as recipe\n",
    "            is_recipe, website = find_website(soup)\n",
    "            #print('Recipe Analysed: '+soup.title.string)\n",
    "            #print('filename: '+filename)\n",
    "\n",
    "            if is_recipe:\n",
    "\n",
    "                #tags, ingredients = analyse_page(soup, website)\n",
    "                tags, ingredients, rating, servings= analyse_page(soup, website)\n",
    "                if ingredients:\n",
    "                    \n",
    "                    \n",
    "                    has_meat, meat_ingredients, meat_ingredients_str = contains_meat_ingredients(ingredients, meat_products)\n",
    "\n",
    "                    if has_meat:\n",
    "\n",
    "                        #Extract meat ingredients and quantities in kg\n",
    "                        ingredient_quant_kg = extract_meat(meat_ingredients_str)\n",
    "                        ingredient_quant_norm = normalize_servings(ingredient_quant_kg, servings)\n",
    "                        #calculate carbon footprint\n",
    "                        fp_per_ingredient, fp_total = compute_carbon_fp(meat_ingredients, ingredient_quant_norm, carbon_footprint_list)\n",
    "                    else:\n",
    "                        ingredient_quant_kg = 0\n",
    "                        ingredient_quant_norm = 0\n",
    "                        fp_per_ingredient = 0\n",
    "                        fp_total = 0\n",
    "\n",
    "                    if verbose: \n",
    "                        #print('Recipe Analysed: '+soup.title.string)\n",
    "                        print('{0} Ingredients: '.format(len(ingredients)))\n",
    "                        print('contains meat:'+str(has_meat))\n",
    "                        if has_meat:\n",
    "                            print(meat_ingredients_str)\n",
    "                            print('ingredient_quantity (kg)= ',ingredient_quant_kg)\n",
    "                            print('ingredient quantity (kg) per serving = ', ingredient_quant_norm )\n",
    "                            print('carbon footprint of recipe = ', fp_total)\n",
    "\n",
    "                        print('{0} tags:'.format(len(tags)))\n",
    "                        print(tags)\n",
    "                        print('number of servings= ', servings)\n",
    "                        print('rating = ', rating)\n",
    "                            \n",
    "                    data.append([soup.title.string, has_meat, meat_ingredients, ingredient_quant_norm, fp_total, rating, website])\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "                #print('not a recipe')\n",
    "        except:\n",
    "            count_exceptions=count_exceptions+1\n",
    "    \n",
    "    \n",
    "    printProgressBar(step, total_num, prefix = 'Progress:', suffix = 'Complete', length = 50)    \n",
    "    \n",
    "    #REMOVE IF YOU WANT MORE SAMPLES\n",
    "    if step%1000==0: #every 100 files, save progress\n",
    "        column_labels=['Recipe Title', 'Has meat', 'Meat types', 'Meat quantity (kg)', 'Carbon footprint','Rating','Website']#'Tags']#missing: 'Carbon footprint', 'Rating', 'Tags'\n",
    "        recipes_df = pd.DataFrame(data, columns = column_labels)\n",
    "        recipes_df.to_csv(DATA_FOLDER+'/recipes_data.csv',  index=False)\n",
    "        #print('saving files, iteration =',step)\n",
    "\n",
    "    step=step+1 #next step\n",
    "    if step>total_num: #next step is beyond dataset\n",
    "        column_labels=['Recipe Title', 'Has meat', 'Meat types', 'Meat quantity (kg)', 'Carbon footprint','Rating','Website']#'Tags']#missing: 'Carbon footprint', 'Rating', 'Tags'\n",
    "        recipes_df = pd.DataFrame(data, columns = column_labels)\n",
    "        recipes_df.to_csv(DATA_FOLDER+'/recipes_data.csv',  index=False) #save all data\n",
    "        break\n",
    "    \n",
    "\n",
    "#column_labels=['Recipe Title', 'Has meat', 'Meat types', 'Meat quantity (kg)', 'Carbon footprint','Rating','Website']#'Tags']#missing: 'Carbon footprint', 'Rating', 'Tags'\n",
    "#recipes_df = pd.DataFrame(data, columns = column_labels)\n",
    "\n",
    "\n",
    "#recipes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the data as csv for in depth analysis\n",
    "#recipes_df.to_csv(DATA_FOLDER+'/recipes_data.csv',  index=False)\n",
    "step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***STOP****\n",
    "All code below this line is experimental and should be deleted in the final version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to test individual files: DELETE IN FINAL VERSION\n",
    "with open(SAMPLE_DATA_FOLDER+'00bb0f80a1286ab29611a34540a8f54e.html') as f:\n",
    "    page = f.read()\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    rate_wrapper=soup.find('span', itemprop='rating')\n",
    "    rate=rate_wrapper.getText()\n",
    "    print(rate_wrapper)\n",
    "    print(rate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Betty crocker ratings are a pain\n",
    "#so are myrecipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#CODE FOR TESTING\n",
    "    \n",
    "            #check webpage and extract ingredients if recognised as recipe\n",
    "    is_recipe, website = find_website(soup)\n",
    "    print('Recipe Analysed: '+soup.title.string)\n",
    "    print('WEBSITE= ', website)\n",
    "    #print(soup.prettify())\n",
    "    tolken_title=tolkenize_title(soup.title.string, website)\n",
    "\n",
    "    #tags, ingredients = analyse_page(soup, website)\n",
    "    tags, ingredients, rating, servings= analyse_page(soup, website)\n",
    "    has_meat, meat_ingredients, meat_ingredients_str = contains_meat_ingredients(ingredients, meat_products)\n",
    "    #Extract meat ingredients and quantities in kg\n",
    "    ingredient_quant_kg = extract_meat(meat_ingredients_str)\n",
    "    print(servings)\n",
    "    print('contains meat:'+str(has_meat))\n",
    "    print(meat_ingredients_str)                  \n",
    "    print('{0} Ingredients: '.format(len(ingredients)))\n",
    "    print(ingredients)\n",
    "    print('{0} tags:'.format(len(tags)))\n",
    "    print(tags)\n",
    "    print('does this recipe contain meat? ', has_meat)\n",
    "    print('meat ingredients=', meat_ingredients_str)\n",
    "    print('ingredient_quantity (kg)= ',ingredient_quant_kg)\n",
    "if has_meat:\n",
    "     #Extract meat ingredients and quantities in kg\n",
    "    ingredient_quant_kg = extract_meat(meat_ingredients_str)\n",
    "    ingredient_quant_norm = normalize_servings(ingredient_quant_kg, servings)\n",
    "                        #calculate carbon footprint\n",
    "    fp_per_ingredient, fp_total = compute_carbon_fp(meat_ingredients, ingredient_quant_norm, carbon_footprint_list)\n",
    "else:\n",
    "    ingredient_quant_kg = 0\n",
    "    ingredient_quant_norm = 0\n",
    "if verbose: \n",
    "    #print('Recipe Analysed: '+soup.title.string)                        \n",
    "    print('tolkenized title : ', tolken_title)\n",
    "\n",
    "                        \n",
    "    print('contains meat:'+str(has_meat))\n",
    "    print(meat_ingredients_str)\n",
    "                        \n",
    "    print('{0} Ingredients: '.format(len(ingredients)))\n",
    "                        #print(ingredients)\n",
    "\n",
    "    print('{0} tags:'.format(len(tags)))\n",
    "    print(tags)\n",
    "    print('number of servings= ', servings)\n",
    "    print('does this recipe contain meat? ', has_meat)\n",
    "                        #print('ingredients = ',ingredients)\n",
    "    print('meat ingredients=', meat_ingredients_str)\n",
    "    print('ingredient_quantity (kg)= ',ingredient_quant_kg)\n",
    "    print('ingredient quantity (kg) per serving = ', ingredient_quant_norm )\n",
    "    print('carbon footprint of recipe = ', fp_total)\n",
    "                            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
